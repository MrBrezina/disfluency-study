{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert and evaluate raw data\n",
    "\n",
    "Basic processing to convert data from its raw form\n",
    "returned by the website to a format useful for statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# set up a DataFrame to collect the processed data\n",
    "columns = [\n",
    "    \"StudyID\", \"ParticipantID\", \"Fluent\", \"Training\",\n",
    "    \"TestID\", \"Type\", \"TrialID\",\n",
    "    \"Font\", \"Sample\", \"Category\",\n",
    "    \"Response\", \"Correct\", \"Seen\", \"Foil\", \"RT\", \"RTnorm\",\n",
    "    \"JoM\", \"JoL\", \"Date\",\n",
    "]\n",
    "d = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data from the raw format to stats-ready format\n",
    "\n",
    "The raw format has all responses from one participant in a single row\n",
    "this breaks down results for individual trials (saved in columns like “test_1_lexical”)\n",
    "and saves these as individual rows.\n",
    "\n",
    "Deal with some minor format differences as the formatting evolved with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 15768 responses from 219 participants.\n"
     ]
    }
   ],
   "source": [
    "# Warning: this takes quite a while to compute\n",
    "\n",
    "# participant counter (Participant ID)\n",
    "pid = 0\n",
    "# counter for trials within each session of a single participant\n",
    "x = 0\n",
    "for fn in glob.glob(os.path.join(\"..\", \"data\", \"raw\", \"*.csv\")):\n",
    "    raw = pd.read_csv(fn)\n",
    "    for i, rraw in raw.iterrows():\n",
    "        # collect data that will be shared across all rows\n",
    "        # for one participant\n",
    "        shared = pd.Series(index=d.columns, dtype=\"float64\")\n",
    "        if \"studyid\" in rraw:\n",
    "            shared[\"StudyID\"] = rraw[\"studyid\"]\n",
    "        else:\n",
    "            shared[\"StudyID\"] = 0  # pilot study\n",
    "        shared[\"ParticipantID\"] = pid\n",
    "        if \"Fluent\" in rraw:\n",
    "            shared[\"Fluent\"] = rraw[\"Fluent\"]\n",
    "        # deal with legacy column names\n",
    "        if \"Native\" in rraw:\n",
    "            shared[\"Fluent\"] = rraw[\"Native\"]\n",
    "        if \"Designer\" in rraw:\n",
    "            shared[\"Training\"] = rraw[\"Designer\"]\n",
    "        if \"Design_skills\" in rraw:\n",
    "            shared[\"Training\"] = rraw[\"Design_skills\"]\n",
    "        for c in rraw.index:\n",
    "            # get values from columns like this: test_1_lexical_5\n",
    "            # ignore values from columns like this: test_1_remember\n",
    "            # or test_1_legibility\n",
    "            if c.startswith(\"test_\") and \\\n",
    "               not (c.endswith(\"_remember\") or c.endswith(\"_legibility\")):\n",
    "                # prefill with shared data\n",
    "                rd = pd.Series(shared)\n",
    "                # set defaults\n",
    "                rd[\"Category\"], rd[\"Seen\"], rd[\"Foil\"] = np.nan, np.nan, np.nan\n",
    "                # get Test ID, Type, and Trial ID from the column name\n",
    "                _, rd[\"TestID\"], rd[\"Type\"], rd[\"TrialID\"] = c.strip().split(\"_\")\n",
    "                # get respond from the value in this column\n",
    "                response = rraw[c].strip().split(\",\")\n",
    "                # tackle legacy formats of responses\n",
    "                # when only some values were provided\n",
    "                rd[\"Font\"] = response[0].strip()\n",
    "                rd[\"Response\"] = response[-2].strip()\n",
    "                rd[\"RT\"] = float(response[-1].strip())\n",
    "                if rd[\"Type\"] == \"lexical\":\n",
    "                    if len(response) == 4:\n",
    "                        rd[\"Sample\"] = response[1].strip()\n",
    "                    else:\n",
    "                        rd[\"Category\"] = response[1].strip()\n",
    "                        rd[\"Sample\"] = response[2].strip()\n",
    "                else:\n",
    "                    if len(response) == 5:\n",
    "                        rd[\"Sample\"] = response[1].strip()\n",
    "                        rd[\"Seen\"] = response[2].strip()\n",
    "                    elif len(response) == 6:\n",
    "                        rd[\"Category\"] = response[1].strip()\n",
    "                        rd[\"Sample\"] = response[2].strip()\n",
    "                        rd[\"Seen\"] = response[3].strip()\n",
    "                    else:\n",
    "                        rd[\"Category\"] = response[1].strip()\n",
    "                        rd[\"Sample\"] = response[2].strip()\n",
    "                        rd[\"Seen\"] = response[3].strip()\n",
    "                        rd[\"Foil\"] = response[4].strip()\n",
    "                # fix legacy values\n",
    "                if isinstance(rd[\"Category\"], str):\n",
    "                    rd[\"Category\"] = rd[\"Category\"].replace(\"nonword\", \"non-word\")\n",
    "                if isinstance(rd[\"Seen\"], str):\n",
    "                    rd[\"Seen\"] = rd[\"Seen\"].replace(\"non-seen\", \"not seen\")\n",
    "                rd[\"Response\"] = rd[\"Response\"].replace(\"non-seen\", \"not seen\")\n",
    "                # add the judgement of learning for this part\n",
    "                # value from column test_1_remember\n",
    "                rd[\"JoM\"] = rraw[\"test_%s_remember\" % rd[\"TestID\"]]\n",
    "                # add the judgement of legibility for this part\n",
    "                # value from column test_1_legibility\n",
    "                rd[\"JoL\"] = rraw[\"test_%s_legibility\" % rd[\"TestID\"]]\n",
    "                rd[\"Date\"] = rraw[-1]\n",
    "                # add a row with for individual trial\n",
    "                d.loc[x] = rd\n",
    "                x += 1\n",
    "        pid += 1\n",
    "# fix types\n",
    "d[\"StudyID\"] = d[\"StudyID\"].astype(int)\n",
    "d[\"ParticipantID\"] = d[\"ParticipantID\"].astype(int)\n",
    "# add normalized RT\n",
    "d[\"RTnorm\"] = np.log(d[\"RT\"])\n",
    "\n",
    "print(\"Processed %d responses from %d participants.\" % (len(d), pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add missing data & evaluate responses\n",
    "\n",
    "Also add response time (RT) transformed using natural logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: this takes quite a while to compute\n",
    "\n",
    "# get a list of words and non-words from txt files used for the website\n",
    "# and map them to their category names (word, non-word)\n",
    "categories = {}\n",
    "for cat in [\"words\", \"non-words\"]:\n",
    "    with open(os.path.join(\"..\", \"data\", \"samples-databases\", cat + \".txt\")) as f:\n",
    "        for w in f.readlines():\n",
    "            categories[w.strip()] = cat[:-1] # remove the final \"s\"\n",
    "\n",
    "# add missing data & evaluate responses\n",
    "for i, rd in d.iterrows():\n",
    "    # convert string \"yes\" to boolean\n",
    "    rd[\"Fluent\"] = (rd[\"Fluent\"] == \"yes\")\n",
    "    if isinstance(rd[\"Category\"], float) or rd[\"Category\"] is np.nan:\n",
    "        # assing correct category if missing\n",
    "        rd[\"Category\"] = categories[rd[\"Sample\"]]\n",
    "    # set Correct to 1 when the participant said sure or probably\n",
    "    # set to zero otherwise\n",
    "    rd[\"Correct\"] = 0\n",
    "    if rd[\"Type\"] == \"lexical\":\n",
    "        if rd[\"Response\"] == (\"Sure \" + rd[\"Category\"]) or \\\n",
    "          rd[\"Response\"] == (\"Probably \" + rd[\"Category\"]):\n",
    "            rd[\"Correct\"] = 1\n",
    "    elif rd[\"Type\"] == \"recognition\":\n",
    "        if rd[\"Response\"] == (\"Sure \" + rd[\"Seen\"]) or \\\n",
    "          rd[\"Response\"] == (\"Probably \" + rd[\"Seen\"]):\n",
    "            rd[\"Correct\"] = 1\n",
    "    d.loc[i] = rd\n",
    "            \n",
    "# add normalized RT\n",
    "d[\"RTnorm\"] = np.log(d[\"RT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data for each part and participant\n",
    "\n",
    "todo\n",
    "Aggregate data for every (study, test, participant) combination.\n",
    "Calculate AUC and RT across all of their relevant responses\n",
    "and average them for: all, words,  and non-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 2 0 1 lexical 1 1\n",
      "error 2 0 1 recognition 2 2\n",
      "error 2 0 2 lexical 1 3\n",
      "error 2 0 2 recognition 2 4\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(2, 0, '1', 'lexical')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.set_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.BaseMultiIndexCodesEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.set_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mget_loc_level\u001b[0;34m(self, key, level, drop_level)\u001b[0m\n\u001b[1;32m   2813\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2814\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2815\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.BaseMultiIndexCodesEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (2, 0, '1', 'lexical')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ce02b163986b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m \u001b[0maggregated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_agg_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-ce02b163986b>\u001b[0m in \u001b[0;36mget_agg_results\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0;31m#print(order, absorder, ttype, \"-\", sid, pid, tid, results.loc[(sid, pid, tid, \"lexical\"), \"Font\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Firstfont\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lexical\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Font\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;31m# fix the type for order column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Order\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;31m# we may have a nested tuples indexer here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_nested_tuple_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_nested_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;31m# we maybe be using a tuple to represent multiple dimensions here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_nested_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m             \u001b[0mcurrent_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1454\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1965\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no slices here, handle elsewhere\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3533\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3535\u001b[0;31m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3536\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3537\u001b[0m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mget_loc_level\u001b[0;34m(self, key, level, drop_level)\u001b[0m\n\u001b[1;32m   2814\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2815\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2816\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2817\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mpartial_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (2, 0, '1', 'lexical')"
     ]
    }
   ],
   "source": [
    "map_JoL = {\n",
    "    \"very easy to read\": 100,\n",
    "    \"easy to read\": 75,\n",
    "    \"ok\": 50,\n",
    "    \"difficult to read\": 25,\n",
    "    \"very difficult to read\": 0,\n",
    "}\n",
    "\n",
    "def get_agg_results(d):\n",
    "    \"\"\"\n",
    "    Aggregate data for every (study, test, participant) combination.\n",
    "    \"\"\"\n",
    "\n",
    "    # prepare empty results DataFrame\n",
    "    # aggregate correct and response time (use mean value)\n",
    "    # keep the rest as is or set NaN value for new columns\n",
    "    result_columns = [\"StudyID\", \"ParticipantID\", \"TestID\", \"Type\", \"Order\", \"Order_absolute\", \"Order_Mary\",\n",
    "                      \"Firstfont\",\n",
    "                      \"Fluent\", \"Training\", \"isDesigner\", \"Font\", \"Correct\", \"RT\", \"RTnorm\",\n",
    "                      \"RT_word\", \"RT_nonword\", \"RTnorm_word\", \"RTnorm_nonword\",\n",
    "                      \"AUC\", \"AUC_word\", \"AUC_nonword\",\n",
    "                      \"JoL\", \"JoM\", \"Date\"]\n",
    "    agg_columns = {k:\"first\" for k in set(d.columns).intersection(result_columns)}\n",
    "    agg_columns[\"Correct\"] = \"mean\"\n",
    "    agg_columns[\"RT\"] = \"mean\"\n",
    "    agg_columns[\"RTnorm\"] = \"mean\"\n",
    "    results = d.groupby([\"StudyID\", \"ParticipantID\", \"TestID\", \"Type\"]).agg(agg_columns)\n",
    "    results = pd.DataFrame(results, columns=result_columns)\n",
    "    results.set_index([\"StudyID\", \"ParticipantID\", \"TestID\", \"Type\"], inplace=True)\n",
    "    # isDesigner is a boolean column to conveniently group designers together\n",
    "    results[\"isDesigner\"] = (results[\"Training\"] != \"Non-designer\")\n",
    "    # convert JoL responses to numerical values\n",
    "    for k, v in map_JoL.items():\n",
    "        results[\"JoL\"] = results[\"JoL\"].astype(str).replace(k, v)\n",
    "    results[\"JoL\"] = results[\"JoL\"].astype(float)\n",
    "\n",
    "    \n",
    "    d[\"TestID\"] = d[\"TestID\"].astype(\"int\")\n",
    "    test_ids = sorted(set(d[\"TestID\"].unique()))\n",
    "    ttypes = sorted(d[\"Type\"].unique())\n",
    "    \n",
    "    # prepare indexes for temporary data frames\n",
    "    # there are two, one based on the Category column used for lexical tasks\n",
    "    # and one based on the Seen column used for recognition\n",
    "    ix = {}\n",
    "    category_name = \"Category\"\n",
    "    categories = [\"word\", \"non-word\"]\n",
    "    responses = [\"Sure word\", \"Probably word\", \"Probably non-word\", \"Sure non-word\"]\n",
    "    ix[\"lexical\"] = (category_name, pd.MultiIndex.from_product([categories, responses], names=[category_name, \"Response\"]))\n",
    "    category_name = \"Seen\"\n",
    "    categories = [\"seen\", \"not seen\"]\n",
    "    responses = [\"Sure seen\", \"Probably seen\", \"Probably not seen\", \"Sure not seen\"]\n",
    "    ix[\"recognition\"] = (category_name, pd.MultiIndex.from_product([categories, responses], names=[category_name, \"Response\"]))\n",
    "\n",
    "    # loop across study IDs, participant IDs, Test IDs, and Test types\n",
    "    # to get each part separately\n",
    "    for sid in d[\"StudyID\"].unique():\n",
    "        for pid in d[d[\"StudyID\"] == sid][\"ParticipantID\"].unique():\n",
    "            for tid in test_ids:\n",
    "                for order, ttype in enumerate(ttypes):\n",
    "                    order += 1  # (0, 1) -> (1, 2)\n",
    "                    absorder  = 2 * (int(tid) - 1) + order # -> (1, 2, 3, 4)\n",
    "                    \n",
    "                    #print(order, absorder, ttype, \"-\", sid, pid, tid)\n",
    "                    \n",
    "                    # subset the data frame to single task-type combination\n",
    "                    # there are four (two parts/tests with two tasks) for each participant in a study\n",
    "                    dtt = d[(d[\"StudyID\"] == sid) & (d[\"ParticipantID\"] == pid) & (d[\"TestID\"] == tid) & (d[\"Type\"] == ttype)]\n",
    "                    \n",
    "                    # get/save the order\n",
    "                    if sid == 1:\n",
    "                        # in study #1 it corresponds to 1 = lexical, 2 = recognition\n",
    "                        results.loc[(sid, pid, tid, ttype), \"Order\"] = order\n",
    "                    else:\n",
    "                        # in study #2 it depends on the Test ID\n",
    "                        if tid == 1:\n",
    "                            results.loc[(sid, pid, tid, ttype), \"Order\"] = order\n",
    "                        elif tid == 2 and order == 1:\n",
    "                            results.loc[(sid, pid, tid, ttype), \"Order\"] = 2\n",
    "                        elif tid == 2 and order == 2:\n",
    "                            results.loc[(sid, pid, tid, ttype), \"Order\"] = 1\n",
    "                        else:\n",
    "                            print(\"This should not happen\", tid, order)\n",
    "                    # absolute order of a part within a session\n",
    "                    results.loc[(sid, pid, tid, ttype), \"Order_absolute\"] = absorder\n",
    "                    try:\n",
    "                        if results[(sid, pid, tid, ttype), \"Order\"] not in [1,2]:\n",
    "                            print(results[(sid, pid, tid, ttype), \"Order\"], sid, pid, tid, ttype, order, absorder)\n",
    "                    except:\n",
    "                        print(\"error\", sid, pid, tid, ttype, order, absorder)\n",
    "                    \n",
    "                    # calculate the AUC\n",
    "                    # figure out which category and index to use for this test type\n",
    "                    # category_name is either “Category” (in lexical task) or “Seen” (in recognition task)\n",
    "                    category_name, index = ix[ttype]\n",
    "                    # get response frequencies for this test type first\n",
    "                    # ensure the order in the index is always the same\n",
    "                    dg = pd.DataFrame(index=index)\n",
    "                    dg[\"Frequencies\"] = dtt.groupby([category_name])[\"Response\"].value_counts()\n",
    "                    dg = dg.fillna(0)\n",
    "                    # use frequencies for word/seen for the y coordinate in the get_auc function\n",
    "                    # use frequencies for non-word/not seen for the x coordinate\n",
    "                    freqs = dg[\"Frequencies\"].tolist()\n",
    "                    auc = get_auc(freqs[4:], freqs[:4])\n",
    "                    results.loc[(sid, pid, tid, ttype), \"AUC\"] = auc\n",
    "                    results.loc[(sid, pid, tid, ttype), \"AUCnorm\"] = normalize_auc(auc)\n",
    "                    correct = results.loc[(sid, pid, tid, ttype), \"Correct\"]\n",
    "                    results.loc[(sid, pid, tid, ttype), \"Correctnorm\"] = normalize_auc(correct)\n",
    "\n",
    "                    # for recognition task only\n",
    "                    # get the mean AUC and RT for words only and non-words only\n",
    "                    if ttype == \"recognition\":\n",
    "                        for cat in [\"word\", \"non-word\"]:\n",
    "                            cat_ = cat.replace(\"-\", \"\")\n",
    "                            dg[\"Frequencies\"] = dtt[dtt[\"Category\"] == cat].groupby([category_name])[\"Response\"].value_counts()\n",
    "                            dg = dg.fillna(0)\n",
    "                            freqs = dg[\"Frequencies\"].tolist()\n",
    "                            auc = get_auc(freqs[4:], freqs[:4])\n",
    "                            results.loc[(sid, pid, tid, ttype), \"AUC_%s\" % cat_] = auc\n",
    "                            results.loc[(sid, pid, tid, ttype), \"AUCnorm_%s\" % cat_] = normalize_auc(auc)\n",
    "                        rt = dtt[dtt[\"Category\"] == cat][\"RT\"].mean()\n",
    "                        results.loc[(sid, pid, tid, ttype), \"RT_%s\" % cat_] = rt\n",
    "                        results.loc[(sid, pid, tid, ttype), \"RTnorm_%s\" % cat_] = normalize_rt(rt)\n",
    "                    \n",
    "                    #print(order, absorder, ttype, \"-\", sid, pid, tid, results.loc[(sid, pid, tid, \"lexical\"), \"Font\"])\n",
    "            results.loc[(sid, pid), \"Firstfont\"] = results.loc[(sid, pid, \"1\", \"lexical\"), \"Font\"]\n",
    "    # fix the type for order column\n",
    "    print(set(results[\"Order\"].unique()))\n",
    "    results[\"Order\"] = results[\"Order\"].astype(int)\n",
    "    # swap recognition tests for SID == 2\n",
    "    # to have lexical and recognition next to each other\n",
    "    for pid in d[d[\"StudyID\"] == 2][\"ParticipantID\"].unique():\n",
    "        backup = results.loc[(2, pid, 1, \"recognition\")].copy()\n",
    "        results.loc[(2, pid, 1, \"recognition\")] = results.loc[(2, pid, 2, \"recognition\")]\n",
    "        results.loc[(2, pid, 2, \"recognition\")] = backup\n",
    "    results.reset_index(inplace=True)\n",
    "    return results\n",
    "\n",
    "aggregated = get_agg_results(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed and aggregated data\n",
    "d.to_csv(os.path.join(\"..\", \"data\", \"data.csv\"))\n",
    "aggregated.to_csv(os.path.join(\"..\", \"data\", \"data_aggregated.csv\"))\n",
    "print(\"Successfully saved to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not used: remove outliers\n",
    "\n",
    "Outliers are responses times (RTs) outside mean +- 2*STD.\n",
    "These RTs are being replaced by a mean of RTs for a participant and task type (lexical, decision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_outliers(d):\n",
    "    \"\"\"\n",
    "    Replace responses outside mean +- 2*STD with a mean\n",
    "    \"\"\"\n",
    "    \n",
    "    means = d.groupby([\"ParticipantID\", \"Type\"])[\"RTnorm\"].mean()\n",
    "    stds = d.groupby([\"ParticipantID\", \"Type\"])[\"RTnorm\"].std()\n",
    "    meanorigs = d.groupby([\"ParticipantID\", \"Type\"])[\"RT\"].mean()\n",
    "    d.set_index([\"ParticipantID\", \"Type\", \"TestID\", \"TrialID\"], inplace=True)\n",
    "    count = 0\n",
    "    # traverse individual responses\n",
    "    for ix, dt in d.iterrows():\n",
    "        # get mean and std for participant and task type\n",
    "        mean = means[ix[:-2]]\n",
    "        std = stds[ix[:-2]]\n",
    "        meanorig = meanorigs[ix[:-2]]\n",
    "        # judge by RTnorm, but replace both RTnorm and RT\n",
    "        if dt[\"RTnorm\"] >= (mean + 2*std):\n",
    "            count += 1\n",
    "            d.loc[ix, \"RT\"] = meanorig\n",
    "            d.loc[ix, \"RTnorm\"] = normalize_rt(meanorig)\n",
    "    d.reset_index(inplace=True)\n",
    "    print(\"Replaced %d outliers\" % count)\n",
    "    return d\n",
    "\n",
    "dwo = replace_outliers(d.copy())\n",
    "aggregatedwo = get_agg_results(dwo)\n",
    "\n",
    "print(\"Replacing outliers changes the global normalized mean from %.3f to %.3f\" % (d[\"RTnorm\"].mean(), dwo[\"RTnorm\"].mean()))\n",
    "print(\"Replacing outliers changes the global mean from %.3f to %.3f\" % (d[\"RT\"].mean(), dwo[\"RT\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed and aggregated data\n",
    "dwo.to_csv(os.path.join(\"..\", \"data\", \"data_outliers-replaced.csv\"))\n",
    "aggregatedwo.to_csv(os.path.join(\"..\", \"data\", \"data_outliers-replaced_aggregated.csv\"))\n",
    "print(\"Successfully saved to CSV files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
