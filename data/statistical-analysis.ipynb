{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 10728 lines.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "datafilename = \"data_processed.csv\"\n",
    "d = pd.read_csv(datafilename)\n",
    "#d = d[-72:] # use just the data for the  last participant\n",
    "\n",
    "print(\"Loading\", len(d), \"lines.\")\n",
    "\n",
    "# functions to calculate AUC and plot ROC\n",
    "\n",
    "def cummulative(x):\n",
    "    return [sum(x[0:i+1]) for i in range(len(x))]\n",
    "\n",
    "def get_auc(x, y):\n",
    "    # make cummulative\n",
    "    x, y = cummulative(x), cummulative(y)\n",
    "    # normalize\n",
    "    x = [xi/max(x) for xi in x]\n",
    "    y = [yi/max(y) for yi in y]\n",
    "    auc = 0\n",
    "    x1, y1 = 0, 0\n",
    "    for x2, y2 in zip(x, y):\n",
    "        auc += (x2 - x1) * (y1 + y2) / 2\n",
    "        x1, y1 = x2, y2\n",
    "    return auc\n",
    "\n",
    "def plot_roc(x, y):\n",
    "    x, y = cummulative(x), cummulative(y)\n",
    "    plt.plot((min(x), max(x)), (min(y), max(y)), color='red', linewidth=1, linestyle='--')\n",
    "    for i in range(len(x)-1):\n",
    "        plt.plot((x[i], x[i+1]), (y[i], y[i+1]), color='black', linewidth=1)\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i], marker='o', s=30, color='black')\n",
    "    #plt.xlim(0.0, 1.0)\n",
    "    #plt.ylim(0.0, 1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## Counts for study ID: 1\n",
      "\n",
      "Number of participants:\n",
      "86.0\n",
      "\n",
      "Fluent vs. non-fluent:\n",
      "True     82.0\n",
      "False     4.0\n",
      "Name: Fluent, dtype: float64\n",
      "\n",
      "Different kinds of designers:\n",
      "Non-designer        42.0\n",
      "Letter designer     24.0\n",
      "Graphic designer    11.0\n",
      "Typographer          5.0\n",
      "Other designer       4.0\n",
      "Name: Designer, dtype: float64\n",
      "\n",
      "Which font was first:\n",
      "sansforgetica    47.0\n",
      "arial            39.0\n",
      "Name: Font, dtype: float64\n",
      "\n",
      "JoM for categories of designers:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Designer\n",
       "Graphic designer    57.590909\n",
       "Letter designer     55.187500\n",
       "Non-designer        55.261905\n",
       "Other designer      50.625000\n",
       "Typographer         61.000000\n",
       "Name: JoM, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JoL for categories of designers:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Designer          JoL                   \n",
       "Graphic designer  very easy to read         10.0\n",
       "                  ok                         6.0\n",
       "                  easy to read               3.0\n",
       "                  very difficult to read     2.0\n",
       "                  difficult to read          1.0\n",
       "Letter designer   very easy to read         14.0\n",
       "                  difficult to read         12.0\n",
       "                  ok                        11.0\n",
       "                  easy to read               8.0\n",
       "                  very difficult to read     3.0\n",
       "Non-designer      very easy to read         37.0\n",
       "                  ok                        22.0\n",
       "                  difficult to read         12.0\n",
       "                  easy to read              11.0\n",
       "                  very difficult to read     2.0\n",
       "Other designer    ok                         4.0\n",
       "                  very easy to read          3.0\n",
       "                  difficult to read          1.0\n",
       "Typographer       very easy to read          4.0\n",
       "                  difficult to read          3.0\n",
       "                  ok                         2.0\n",
       "                  easy to read               1.0\n",
       "Name: JoL, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## Counts for study ID: 2\n",
      "\n",
      "Number of participants:\n",
      "63.0\n",
      "\n",
      "Fluent vs. non-fluent:\n",
      "True     61.0\n",
      "False     2.0\n",
      "Name: Fluent, dtype: float64\n",
      "\n",
      "Different kinds of designers:\n",
      "Non-designer        43.0\n",
      "Letter designer      8.0\n",
      "Graphic designer     5.0\n",
      "Typographer          4.0\n",
      "Other designer       3.0\n",
      "Name: Designer, dtype: float64\n",
      "\n",
      "Which font was first:\n",
      "sansforgetica    35.0\n",
      "arial            28.0\n",
      "Name: Font, dtype: float64\n",
      "\n",
      "JoM for categories of designers:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Designer\n",
       "Graphic designer    55.900000\n",
       "Letter designer     60.312500\n",
       "Non-designer        51.813953\n",
       "Other designer      48.666667\n",
       "Typographer         53.625000\n",
       "Name: JoM, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JoL for categories of designers:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Designer          JoL                   \n",
       "Graphic designer  very easy to read          5.0\n",
       "                  difficult to read          3.0\n",
       "                  easy to read               1.0\n",
       "                  ok                         1.0\n",
       "Letter designer   difficult to read          6.0\n",
       "                  easy to read               5.0\n",
       "                  very easy to read          3.0\n",
       "                  ok                         1.0\n",
       "                  very difficult to read     1.0\n",
       "Non-designer      very easy to read         38.0\n",
       "                  ok                        19.0\n",
       "                  difficult to read         13.0\n",
       "                  easy to read              10.0\n",
       "                  very difficult to read     6.0\n",
       "Other designer    difficult to read          3.0\n",
       "                  easy to read               2.0\n",
       "                  very easy to read          1.0\n",
       "Typographer       ok                         4.0\n",
       "                  easy to read               2.0\n",
       "                  very easy to read          2.0\n",
       "Name: JoL, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TPP = 72  # there are 72 trials per participant\n",
    "TPT = 36  # there are 36 trials per test\n",
    "TPL = 20  # there are 20 trials per lexical test\n",
    "\n",
    "for sid in [1, 2]:\n",
    "    dt = d[d[\"Study ID\"] == sid]\n",
    "    if not dt.empty:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"## Counts for study ID:\", sid)\n",
    "        print()\n",
    "        print(\"Number of participants:\")\n",
    "        print(len(dt) / TPP) \n",
    "        print()\n",
    "        print(\"Fluent vs. non-fluent:\")\n",
    "        print(dt[\"Fluent\"].value_counts() / TPP)\n",
    "        print()\n",
    "        print(\"Different kinds of designers:\")\n",
    "        print(dt[\"Designer\"].value_counts() / TPP)\n",
    "        print()\n",
    "        print(\"Which font was first:\")\n",
    "        print(dt[(dt[\"Test ID\"] == 1) & (dt[\"Test type\"] == \"lexical\")][\"Font\"].value_counts() / TPL)\n",
    "        print()\n",
    "        print(\"JoM for categories of designers:\")\n",
    "        display(dt.groupby(\"Designer\")[\"JoM\"].mean())\n",
    "        print()\n",
    "        print(\"JoL for categories of designers:\")\n",
    "        display(dt.groupby(\"Designer\")[\"JoL\"].value_counts() / TPT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating AUC and RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Native</th>\n",
       "      <th>Designer</th>\n",
       "      <th>Font</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Response time</th>\n",
       "      <th>Response time (word)</th>\n",
       "      <th>Response time (non-word)</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AUC (word)</th>\n",
       "      <th>AUC (non-word)</th>\n",
       "      <th>JoL</th>\n",
       "      <th>JoM</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Study ID</th>\n",
       "      <th>Participant ID</th>\n",
       "      <th>Test ID</th>\n",
       "      <th>Test type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2.0</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">10656.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>lexical</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Non-designer</td>\n",
       "      <td>arial</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>2817.600</td>\n",
       "      <td>2083.300</td>\n",
       "      <td>3551.900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>easy to read</td>\n",
       "      <td>50</td>\n",
       "      <td>12-04-2019 09:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recognition</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Non-designer</td>\n",
       "      <td>sansforgetica</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>3352.125</td>\n",
       "      <td>3185.250</td>\n",
       "      <td>3519.000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.93750</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>easy to read</td>\n",
       "      <td>50</td>\n",
       "      <td>12-04-2019 09:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>lexical</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Non-designer</td>\n",
       "      <td>sansforgetica</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2655.300</td>\n",
       "      <td>1993.700</td>\n",
       "      <td>3316.900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>difficult to read</td>\n",
       "      <td>46</td>\n",
       "      <td>12-04-2019 09:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recognition</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Non-designer</td>\n",
       "      <td>arial</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>2763.000</td>\n",
       "      <td>2567.125</td>\n",
       "      <td>2958.875</td>\n",
       "      <td>0.648438</td>\n",
       "      <td>0.65625</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>difficult to read</td>\n",
       "      <td>46</td>\n",
       "      <td>12-04-2019 09:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Native      Designer  \\\n",
       "Study ID Participant ID Test ID Test type                           \n",
       "2.0      10656.0        1       lexical         NaN  Non-designer   \n",
       "                                recognition     NaN  Non-designer   \n",
       "                        2       lexical         NaN  Non-designer   \n",
       "                                recognition     NaN  Non-designer   \n",
       "\n",
       "                                                      Font  Correct  \\\n",
       "Study ID Participant ID Test ID Test type                             \n",
       "2.0      10656.0        1       lexical              arial   0.9500   \n",
       "                                recognition  sansforgetica   0.7500   \n",
       "                        2       lexical      sansforgetica   1.0000   \n",
       "                                recognition          arial   0.5625   \n",
       "\n",
       "                                             Response time  \\\n",
       "Study ID Participant ID Test ID Test type                    \n",
       "2.0      10656.0        1       lexical           2817.600   \n",
       "                                recognition       3352.125   \n",
       "                        2       lexical           2655.300   \n",
       "                                recognition       2763.000   \n",
       "\n",
       "                                             Response time (word)  \\\n",
       "Study ID Participant ID Test ID Test type                           \n",
       "2.0      10656.0        1       lexical                  2083.300   \n",
       "                                recognition              3185.250   \n",
       "                        2       lexical                  1993.700   \n",
       "                                recognition              2567.125   \n",
       "\n",
       "                                             Response time (non-word)  \\\n",
       "Study ID Participant ID Test ID Test type                               \n",
       "2.0      10656.0        1       lexical                      3551.900   \n",
       "                                recognition                  3519.000   \n",
       "                        2       lexical                      3316.900   \n",
       "                                recognition                  2958.875   \n",
       "\n",
       "                                                  AUC  AUC (word)  \\\n",
       "Study ID Participant ID Test ID Test type                           \n",
       "2.0      10656.0        1       lexical      1.000000         NaN   \n",
       "                                recognition  0.812500     0.93750   \n",
       "                        2       lexical      1.000000         NaN   \n",
       "                                recognition  0.648438     0.65625   \n",
       "\n",
       "                                             AUC (non-word)  \\\n",
       "Study ID Participant ID Test ID Test type                     \n",
       "2.0      10656.0        1       lexical                 NaN   \n",
       "                                recognition         0.75000   \n",
       "                        2       lexical                 NaN   \n",
       "                                recognition         0.71875   \n",
       "\n",
       "                                                           JoL  JoM  \\\n",
       "Study ID Participant ID Test ID Test type                             \n",
       "2.0      10656.0        1       lexical           easy to read   50   \n",
       "                                recognition       easy to read   50   \n",
       "                        2       lexical      difficult to read   46   \n",
       "                                recognition  difficult to read   46   \n",
       "\n",
       "                                                         Date  \n",
       "Study ID Participant ID Test ID Test type                      \n",
       "2.0      10656.0        1       lexical      12-04-2019 09:34  \n",
       "                                recognition  12-04-2019 09:34  \n",
       "                        2       lexical      12-04-2019 09:34  \n",
       "                                recognition  12-04-2019 09:34  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for every (study, test, participant) combination\n",
    "# calculate AUC for their responses to all the relevant trials (i.e. 36 trials in a test)\n",
    "# and the averge response time\n",
    "\n",
    "result_columns = [\"Study ID\", \"Participant ID\", \"Test ID\", \"Test type\",\n",
    "                  \"Native\", \"Designer\", \"Font\", \"Correct\", \"Response time\",\n",
    "                  \"Response time (word)\", \"Response time (non-word)\",\n",
    "                  \"AUC\", \"AUC (word)\", \"AUC (non-word)\",\n",
    "                  \"JoL\", \"JoM\", \"Date\"]\n",
    "agg_columns = {k:\"first\" for k in set(d.columns).intersection(result_columns)}\n",
    "agg_columns[\"Correct\"] = \"mean\"\n",
    "agg_columns[\"Response time\"] = \"mean\"\n",
    "results = d.groupby([\"Study ID\", \"Participant ID\", \"Test ID\", \"Test type\"]).agg(agg_columns)\n",
    "results = pd.DataFrame(results, columns=result_columns)\n",
    "results.set_index([\"Study ID\", \"Participant ID\", \"Test ID\", \"Test type\"], inplace=True)\n",
    "\n",
    "# prepare indexes for temporary data frames\n",
    "ix = {}\n",
    "test_ids = set(d[\"Test ID\"].unique())\n",
    "category_used = \"Category\"\n",
    "categories = [\"word\", \"non-word\"]\n",
    "responses = [\"Sure word\", \"Probably word\", \"Probably non-word\", \"Sure non-word\"]\n",
    "ix[\"lexical\"] = (category_used, pd.MultiIndex.from_product([categories, responses], names=[category_used, \"Response\"]))\n",
    "category_used = \"Seen\"\n",
    "categories = [\"seen\", \"not seen\"]\n",
    "responses = [\"Sure seen\", \"Probably seen\", \"Probably not seen\", \"Sure not seen\"]\n",
    "ix[\"recognition\"] = (category_used, pd.MultiIndex.from_product([categories, responses], names=[category_used, \"Response\"]))\n",
    "\n",
    "for sid in d[\"Study ID\"].unique():\n",
    "    for pid in d[d[\"Study ID\"] == sid][\"Participant ID\"].unique():\n",
    "        for tid in test_ids:\n",
    "            dt = d[(d[\"Study ID\"] == sid) & (d[\"Participant ID\"] == pid) & (d[\"Test ID\"] == tid)]\n",
    "            for ttype in dt[\"Test type\"].unique():\n",
    "                category_used, index = ix[ttype]\n",
    "                # ensure the order in the index is always the same\n",
    "                dg = pd.DataFrame(index=index)\n",
    "                dg[\"Frequencies\"] = dt[dt[\"Test type\"] == ttype].groupby([category_used])[\"Response\"].value_counts()\n",
    "                dg = dg.fillna(0)\n",
    "                # frequencies for word/seen -> y coordinate\n",
    "                # frequencies for non-word/not seen -> x coordinate\n",
    "                freqs = dg[\"Frequencies\"].tolist()\n",
    "                auc = get_auc(freqs[4:], freqs[:4])\n",
    "                results.loc[(sid, pid, tid, ttype), \"AUC\"] = auc\n",
    "                rt = dt[dt[\"Test type\"] == ttype][\"Response time\"].mean()\n",
    "                results.loc[(sid, pid, tid, ttype), \"Response time\"] = rt\n",
    "                for cat in [\"word\", \"non-word\"]:\n",
    "                    if ttype == \"recognition\":\n",
    "                        dg[\"Frequencies\"] = dt[(dt[\"Test type\"] == ttype) & (dt[\"Category\"] == cat)].groupby([category_used])[\"Response\"].value_counts()\n",
    "                        dg = dg.fillna(0)\n",
    "                        freqs = dg[\"Frequencies\"].tolist()\n",
    "                        auc = get_auc(freqs[4:], freqs[:4])\n",
    "                        results.loc[(sid, pid, tid, ttype), \"AUC (%s)\" % cat] = auc\n",
    "                    rt = dt[(dt[\"Test type\"] == ttype) & (dt[\"Category\"] == cat)][\"Response time\"].mean()\n",
    "                    results.loc[(sid, pid, tid, ttype), \"Response time (%s)\" % cat] = rt\n",
    "            \n",
    "display(results[-4:])\n",
    "results.to_csv(datafilename.replace(\"_processed.csv\", \"_stats.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Native</th>\n",
       "      <th>Designer</th>\n",
       "      <th>Font</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Response time</th>\n",
       "      <th>Response time (word)</th>\n",
       "      <th>Response time (non-word)</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AUC (word)</th>\n",
       "      <th>AUC (non-word)</th>\n",
       "      <th>JoL</th>\n",
       "      <th>JoM</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Study ID</th>\n",
       "      <th>Participant ID</th>\n",
       "      <th>Test ID</th>\n",
       "      <th>Test type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2.0</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">10656.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>lexical</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Non-designer</td>\n",
       "      <td>arial</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>2817.600</td>\n",
       "      <td>2083.300</td>\n",
       "      <td>3551.900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>easy to read</td>\n",
       "      <td>50</td>\n",
       "      <td>12-04-2019 09:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recognition</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Non-designer</td>\n",
       "      <td>sansforgetica</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>3352.125</td>\n",
       "      <td>3185.250</td>\n",
       "      <td>3519.000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.93750</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>easy to read</td>\n",
       "      <td>50</td>\n",
       "      <td>12-04-2019 09:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>lexical</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Non-designer</td>\n",
       "      <td>sansforgetica</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2655.300</td>\n",
       "      <td>1993.700</td>\n",
       "      <td>3316.900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>difficult to read</td>\n",
       "      <td>46</td>\n",
       "      <td>12-04-2019 09:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recognition</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Non-designer</td>\n",
       "      <td>arial</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>2763.000</td>\n",
       "      <td>2567.125</td>\n",
       "      <td>2958.875</td>\n",
       "      <td>0.648438</td>\n",
       "      <td>0.65625</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>difficult to read</td>\n",
       "      <td>46</td>\n",
       "      <td>12-04-2019 09:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Native      Designer  \\\n",
       "Study ID Participant ID Test ID Test type                           \n",
       "2.0      10656.0        1       lexical         NaN  Non-designer   \n",
       "                                recognition     NaN  Non-designer   \n",
       "                        2       lexical         NaN  Non-designer   \n",
       "                                recognition     NaN  Non-designer   \n",
       "\n",
       "                                                      Font  Correct  \\\n",
       "Study ID Participant ID Test ID Test type                             \n",
       "2.0      10656.0        1       lexical              arial   0.9500   \n",
       "                                recognition  sansforgetica   0.7500   \n",
       "                        2       lexical      sansforgetica   1.0000   \n",
       "                                recognition          arial   0.5625   \n",
       "\n",
       "                                             Response time  \\\n",
       "Study ID Participant ID Test ID Test type                    \n",
       "2.0      10656.0        1       lexical           2817.600   \n",
       "                                recognition       3352.125   \n",
       "                        2       lexical           2655.300   \n",
       "                                recognition       2763.000   \n",
       "\n",
       "                                             Response time (word)  \\\n",
       "Study ID Participant ID Test ID Test type                           \n",
       "2.0      10656.0        1       lexical                  2083.300   \n",
       "                                recognition              3185.250   \n",
       "                        2       lexical                  1993.700   \n",
       "                                recognition              2567.125   \n",
       "\n",
       "                                             Response time (non-word)  \\\n",
       "Study ID Participant ID Test ID Test type                               \n",
       "2.0      10656.0        1       lexical                      3551.900   \n",
       "                                recognition                  3519.000   \n",
       "                        2       lexical                      3316.900   \n",
       "                                recognition                  2958.875   \n",
       "\n",
       "                                                  AUC  AUC (word)  \\\n",
       "Study ID Participant ID Test ID Test type                           \n",
       "2.0      10656.0        1       lexical      1.000000         NaN   \n",
       "                                recognition  0.812500     0.93750   \n",
       "                        2       lexical      1.000000         NaN   \n",
       "                                recognition  0.648438     0.65625   \n",
       "\n",
       "                                             AUC (non-word)  \\\n",
       "Study ID Participant ID Test ID Test type                     \n",
       "2.0      10656.0        1       lexical                 NaN   \n",
       "                                recognition         0.75000   \n",
       "                        2       lexical                 NaN   \n",
       "                                recognition         0.71875   \n",
       "\n",
       "                                                           JoL  JoM  \\\n",
       "Study ID Participant ID Test ID Test type                             \n",
       "2.0      10656.0        1       lexical           easy to read   50   \n",
       "                                recognition       easy to read   50   \n",
       "                        2       lexical      difficult to read   46   \n",
       "                                recognition  difficult to read   46   \n",
       "\n",
       "                                                         Date  \n",
       "Study ID Participant ID Test ID Test type                      \n",
       "2.0      10656.0        1       lexical      12-04-2019 09:34  \n",
       "                                recognition  12-04-2019 09:34  \n",
       "                        2       lexical      12-04-2019 09:34  \n",
       "                                recognition  12-04-2019 09:34  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove outliers\n",
    "\n",
    "d = d[np.abs(d[\"Response time\"]-d[\"Response time\"].mean()) <= 3*d[\"Response time\"].std()]\n",
    "\n",
    "\n",
    "\n",
    "# for every (study, test, participant) combination\n",
    "# calculate AUC for their responses to all the relevant trials (i.e. 36 trials in a test)\n",
    "# and the averge response time\n",
    "\n",
    "result_columns = [\"Study ID\", \"Participant ID\", \"Test ID\", \"Test type\",\n",
    "                  \"Native\", \"Designer\", \"Font\", \"Correct\", \"Response time\",\n",
    "                  \"Response time (word)\", \"Response time (non-word)\",\n",
    "                  \"AUC\", \"AUC (word)\", \"AUC (non-word)\",\n",
    "                  \"JoL\", \"JoM\", \"Date\"]\n",
    "agg_columns = {k:\"first\" for k in set(d.columns).intersection(result_columns)}\n",
    "agg_columns[\"Correct\"] = \"mean\"\n",
    "agg_columns[\"Response time\"] = \"mean\"\n",
    "results = d.groupby([\"Study ID\", \"Participant ID\", \"Test ID\", \"Test type\"]).agg(agg_columns)\n",
    "results = pd.DataFrame(results, columns=result_columns)\n",
    "results.set_index([\"Study ID\", \"Participant ID\", \"Test ID\", \"Test type\"], inplace=True)\n",
    "\n",
    "# prepare indexes for temporary data frames\n",
    "ix = {}\n",
    "test_ids = set(d[\"Test ID\"].unique())\n",
    "category_used = \"Category\"\n",
    "categories = [\"word\", \"non-word\"]\n",
    "responses = [\"Sure word\", \"Probably word\", \"Probably non-word\", \"Sure non-word\"]\n",
    "ix[\"lexical\"] = (category_used, pd.MultiIndex.from_product([categories, responses], names=[category_used, \"Response\"]))\n",
    "category_used = \"Seen\"\n",
    "categories = [\"seen\", \"not seen\"]\n",
    "responses = [\"Sure seen\", \"Probably seen\", \"Probably not seen\", \"Sure not seen\"]\n",
    "ix[\"recognition\"] = (category_used, pd.MultiIndex.from_product([categories, responses], names=[category_used, \"Response\"]))\n",
    "\n",
    "for sid in d[\"Study ID\"].unique():\n",
    "    for pid in d[d[\"Study ID\"] == sid][\"Participant ID\"].unique():\n",
    "        for tid in test_ids:\n",
    "            dt = d[(d[\"Study ID\"] == sid) & (d[\"Participant ID\"] == pid) & (d[\"Test ID\"] == tid)]\n",
    "            for ttype in dt[\"Test type\"].unique():\n",
    "                category_used, index = ix[ttype]\n",
    "                # ensure the order in the index is always the same\n",
    "                dg = pd.DataFrame(index=index)\n",
    "                dg[\"Frequencies\"] = dt[dt[\"Test type\"] == ttype].groupby([category_used])[\"Response\"].value_counts()\n",
    "                dg = dg.fillna(0)\n",
    "                # frequencies for word/seen -> y coordinate\n",
    "                # frequencies for non-word/not seen -> x coordinate\n",
    "                freqs = dg[\"Frequencies\"].tolist()\n",
    "                auc = get_auc(freqs[4:], freqs[:4])\n",
    "                results.loc[(sid, pid, tid, ttype), \"AUC\"] = auc\n",
    "                rt = dt[dt[\"Test type\"] == ttype][\"Response time\"].mean()\n",
    "                results.loc[(sid, pid, tid, ttype), \"Response time\"] = rt\n",
    "                for cat in [\"word\", \"non-word\"]:\n",
    "                    if ttype == \"recognition\":\n",
    "                        dg[\"Frequencies\"] = dt[(dt[\"Test type\"] == ttype) & (dt[\"Category\"] == cat)].groupby([category_used])[\"Response\"].value_counts()\n",
    "                        dg = dg.fillna(0)\n",
    "                        freqs = dg[\"Frequencies\"].tolist()\n",
    "                        auc = get_auc(freqs[4:], freqs[:4])\n",
    "                        results.loc[(sid, pid, tid, ttype), \"AUC (%s)\" % cat] = auc\n",
    "                    rt = dt[(dt[\"Test type\"] == ttype) & (dt[\"Category\"] == cat)][\"Response time\"].mean()\n",
    "                    results.loc[(sid, pid, tid, ttype), \"Response time (%s)\" % cat] = rt\n",
    "            \n",
    "display(results[-4:])\n",
    "d.to_csv(datafilename.replace(\"_processed.csv\", \"_processed-without-outliers.csv\"))\n",
    "results.to_csv(datafilename.replace(\"_processed.csv\", \"_stats-without-outliers.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 10715 into shape (72)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d8291bb612bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Study ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Participant ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Test ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Test type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Response time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Study ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Participant ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Test ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Test type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m72\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#display(times)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 10715 into shape (72)"
     ]
    }
   ],
   "source": [
    "times = pd.DataFrame(d, columns=[\"Study ID\", \"Participant ID\", \"Test ID\", \"Test type\", \"Response time\"])\n",
    "times.set_index([\"Study ID\", \"Participant ID\", \"Test ID\", \"Test type\"], inplace=True)\n",
    "times = pd.DataFrame(times.values.reshape((-1,72)))\n",
    "#display(times)\n",
    "times.T.plot(figsize=(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times.mean().plot(figsize=(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
