{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert and evaluate raw data\n",
    "\n",
    "Converting the data from its raw form returned by the website to a form better suited for statistical analysis and subsequent aggregation.\n",
    "\n",
    "The processed data looks like this:\n",
    "\n",
    "### Columns used for indexing\n",
    "\n",
    "- `StudyID` (int), 0 refers to pilot study, 1 and 2 refer to studies #1 and #2 which had a different study format (see our report)\n",
    "- `ParticipantID` (int, 0 and higher)\n",
    "- `TestID` (int, 1 or 2) the first or second part/test of the study (this is called a “study part” in our report)\n",
    "- `Type` (“lexical” or “recognition”) identifies a task within a part/test, the lexical task is always the first and the recognition task is always the second\n",
    "- `Trial ID` (int, positive) for individual trials in the non-aggregated data\n",
    "\n",
    "### Columns with responses from the introduction\n",
    "\n",
    "- `Fluent` (bool) whether participant was fluent in English\n",
    "- `Training` (str) one of the training categories\n",
    "\n",
    "### Columns with responses from the trials\n",
    "\n",
    "- `Font` (str) font used, “arial” or ‘sansforgetica”\n",
    "- `Sample` (str) sample text used\n",
    "- `Category` (str) word or non-word\n",
    "- `Response` (str) differs for the lexical (Sure word, Probably word, Sure non-word, Probably non-word) and recognition (Sure seen, Probably seen, Sure not seen, Probably not seen) task\n",
    "- `Correct` (float) whether participants response was correct (1.0) or not (0.0) or **mean** of these values in the aggregated data\n",
    "- `Seen` (bool) for recognition tasks only, whether the item (sample) had been shown in the lexical task\n",
    "- `Foil` (str) for recognition tasks only, fake sample used instead of the real sample for items “not seen” in the lexical task\n",
    "- `RT` (float) response time or **mean** of response times in the aggregated data\n",
    "- `RTnorm` (float) normalized response time or **mean** of normalized response times in the aggregated data\n",
    "\n",
    "### Columns used in aggregated data\n",
    "\n",
    "- all columns above and\n",
    "- `Task ID` (int, 1–4) the order of tasks within a single study session, for the same single participant\n",
    "- `Order` (int, 1 or 2) to specify the order of tasks in a study session\n",
    "- `Firstfont` (str) the first font the participant had seen\n",
    "- `isDesigner` (bool) whether participants belong to one of the categories of designers, based on `Training`\n",
    "- `RT_word` (float) mean response time for words only\n",
    "- `RT_nonword` (float) mean response time for non-words only\n",
    "- `RTnorm_word` (float) mean normalized response time for words only\n",
    "- `RTnorm_nonword` (float) mean normalized response time for non-words only\n",
    "- `AUC` (float) for recognition tasks only, area under curve of participant’s responses in this task\n",
    "- `AUC_word` (float) same as `AUC`, but for words only\n",
    "- `AUC_nonword` (float) same as `AUC`, but for non-words only\n",
    "- `AUCnorm` (float) same as `AUC`, but normalized\n",
    "- `AUCnorm_word` (float) same as `AUC`, but normalized and for words only\n",
    "- `AUCnorm_nonword` (float) same as `AUC`, but normalized and for non-words only\n",
    "- `Correctnorm` (float) same as `Correct`, but normalized in non-aggregated data\n",
    "\n",
    "### Columns with responses from the intervening questionnaires\n",
    "\n",
    "- `JoM` (float) response to judgement of memory for the lexical task (copied over to the following recognition task too)\n",
    "- `JoL` (str) response to judgement of learning for the lexical task (copied over to the following recognition task too), in the aggregated data, the responses are converted to scale 0-100 to get a mean (see `map_JoL` mapping below)\n",
    "\n",
    "### Other columns\n",
    "\n",
    "- `Date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# set up a DataFrame to collect the processed data\n",
    "columns = [\n",
    "    \"StudyID\", \"ParticipantID\", \"Fluent\", \"Training\",\n",
    "    \"TestID\", \"Type\", \"TrialID\",\n",
    "    \"Font\", \"Sample\", \"Category\",\n",
    "    \"Response\", \"Correct\", \"Seen\", \"Foil\", \"RT\", \"RTnorm\",\n",
    "    \"JoM\", \"JoL\", \"Date\",\n",
    "]\n",
    "d = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization functions and calculation of AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_auc(auc):\n",
    "    \"\"\"\n",
    "    Transform the square root of AUC\n",
    "    using arcsin and multiply by 2.\n",
    "    \"\"\"\n",
    "\n",
    "    return 2 * np.arcsin(np.sqrt(auc))\n",
    "\n",
    "\n",
    "def normalize_rt(rt):\n",
    "    \"\"\"\n",
    "    Tranform RTs\n",
    "    using natural logarithm.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.log(rt)\n",
    "\n",
    "\n",
    "def denormalize_auc(aucnorm):\n",
    "    \"\"\"\n",
    "    Transform the normalized AUC back\n",
    "    using a square of the sine value of its half.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sin(aucnorm / 2) ** 2\n",
    "\n",
    "\n",
    "def denormalize_rt(rtnorm):\n",
    "    \"\"\"\n",
    "    Tranform the normalized RTs back\n",
    "    using the exponential function.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.exp(rtnorm)\n",
    "\n",
    "\n",
    "def cummulative(x):\n",
    "    return [sum(x[0:i+1]) for i in range(len(x))]\n",
    "\n",
    "\n",
    "def get_auc(x, y):\n",
    "    # make cummulative\n",
    "    x, y = cummulative(x), cummulative(y)\n",
    "    # normalize\n",
    "    x = [xi/max(x) for xi in x]\n",
    "    y = [yi/max(y) for yi in y]\n",
    "    auc = 0\n",
    "    x1, y1 = 0, 0\n",
    "    for x2, y2 in zip(x, y):\n",
    "        auc += (x2 - x1) * (y1 + y2) / 2\n",
    "        x1, y1 = x2, y2\n",
    "    return auc\n",
    "\n",
    "map_JoL = {\n",
    "    \"very easy to read\": 100,\n",
    "    \"easy to read\": 75,\n",
    "    \"ok\": 50,\n",
    "    \"difficult to read\": 25,\n",
    "    \"very difficult to read\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data from the raw format to stats-ready format\n",
    "\n",
    "The raw format has all responses from one participant in a single row\n",
    "this breaks down results for individual trials (saved in columns like “test_1_lexical”)\n",
    "and saves these as individual rows.\n",
    "\n",
    "Deal with some minor format differences as the formatting evolved with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 15768 responses from 219 participants.\n"
     ]
    }
   ],
   "source": [
    "# Warning: this takes quite a while to compute\n",
    "\n",
    "# participant counter (Participant ID)\n",
    "pid = 0\n",
    "# counter for trials within each session of a single participant\n",
    "x = 0\n",
    "for fn in glob.glob(os.path.join(\"..\", \"data\", \"raw\", \"*.csv\")):\n",
    "    raw = pd.read_csv(fn)\n",
    "    for i, rraw in raw.iterrows():\n",
    "        # collect data that will be shared across all rows\n",
    "        # for one participant\n",
    "        shared = pd.Series(index=d.columns, dtype=\"float64\")\n",
    "        if \"studyid\" in rraw:\n",
    "            shared[\"StudyID\"] = rraw[\"studyid\"]\n",
    "        else:\n",
    "            shared[\"StudyID\"] = 0  # pilot study\n",
    "        shared[\"ParticipantID\"] = pid\n",
    "        if \"Fluent\" in rraw:\n",
    "            shared[\"Fluent\"] = rraw[\"Fluent\"]\n",
    "        # deal with legacy column names\n",
    "        if \"Native\" in rraw:\n",
    "            shared[\"Fluent\"] = rraw[\"Native\"]\n",
    "        if \"Designer\" in rraw:\n",
    "            shared[\"Training\"] = rraw[\"Designer\"]\n",
    "        if \"Design_skills\" in rraw:\n",
    "            shared[\"Training\"] = rraw[\"Design_skills\"]\n",
    "        for c in rraw.index:\n",
    "            # get values from columns like this: test_1_lexical_5\n",
    "            # ignore values from columns like this: test_1_remember\n",
    "            # or test_1_legibility\n",
    "            if c.startswith(\"test_\") and \\\n",
    "               not (c.endswith(\"_remember\") or c.endswith(\"_legibility\")):\n",
    "                # prefill with shared data\n",
    "                rd = pd.Series(shared)\n",
    "                # set defaults\n",
    "                rd[\"Category\"], rd[\"Seen\"], rd[\"Foil\"] = np.nan, np.nan, np.nan\n",
    "                # get Test ID, Type, and Trial ID from the column name\n",
    "                _, rd[\"TestID\"], rd[\"Type\"], rd[\"TrialID\"] = c.strip().split(\"_\")\n",
    "                # get respond from the value in this column\n",
    "                response = rraw[c].strip().split(\",\")\n",
    "                # tackle legacy formats of responses\n",
    "                # when only some values were provided\n",
    "                rd[\"Font\"] = response[0].strip()\n",
    "                rd[\"Response\"] = response[-2].strip()\n",
    "                rd[\"RT\"] = float(response[-1].strip())\n",
    "                if rd[\"Type\"] == \"lexical\":\n",
    "                    if len(response) == 4:\n",
    "                        rd[\"Sample\"] = response[1].strip()\n",
    "                    else:\n",
    "                        rd[\"Category\"] = response[1].strip()\n",
    "                        rd[\"Sample\"] = response[2].strip()\n",
    "                else:\n",
    "                    if len(response) == 5:\n",
    "                        rd[\"Sample\"] = response[1].strip()\n",
    "                        rd[\"Seen\"] = response[2].strip()\n",
    "                    elif len(response) == 6:\n",
    "                        rd[\"Category\"] = response[1].strip()\n",
    "                        rd[\"Sample\"] = response[2].strip()\n",
    "                        rd[\"Seen\"] = response[3].strip()\n",
    "                    else:\n",
    "                        rd[\"Category\"] = response[1].strip()\n",
    "                        rd[\"Sample\"] = response[2].strip()\n",
    "                        rd[\"Seen\"] = response[3].strip()\n",
    "                        rd[\"Foil\"] = response[4].strip()\n",
    "                # fix legacy values\n",
    "                if isinstance(rd[\"Category\"], str):\n",
    "                    rd[\"Category\"] = rd[\"Category\"].replace(\"nonword\", \"non-word\")\n",
    "                if isinstance(rd[\"Seen\"], str):\n",
    "                    rd[\"Seen\"] = rd[\"Seen\"].replace(\"non-seen\", \"not seen\")\n",
    "                rd[\"Response\"] = rd[\"Response\"].replace(\"non-seen\", \"not seen\")\n",
    "                # add the judgement of learning for this part\n",
    "                # value from column test_1_remember\n",
    "                rd[\"JoM\"] = rraw[\"test_%s_remember\" % rd[\"TestID\"]]\n",
    "                # add the judgement of legibility for this part\n",
    "                # value from column test_1_legibility\n",
    "                rd[\"JoL\"] = rraw[\"test_%s_legibility\" % rd[\"TestID\"]]\n",
    "                rd[\"Date\"] = rraw[-1]\n",
    "                # add a row with for individual trial\n",
    "                d.loc[x] = rd\n",
    "                x += 1\n",
    "        pid += 1\n",
    "# fix types\n",
    "d[\"StudyID\"] = d[\"StudyID\"].astype(int)\n",
    "d[\"ParticipantID\"] = d[\"ParticipantID\"].astype(int)\n",
    "d[\"TestID\"] = d[\"TestID\"].astype(\"int\")\n",
    "\n",
    "print(\"Processed %d responses from %d participants.\" % (len(d), pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add missing data & evaluate responses\n",
    "\n",
    "Evaluat whether responses are correct or not add normalized response time (RT) transformed using natural logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: this takes quite a while to compute\n",
    "\n",
    "# get a list of words and non-words from txt files used for the website\n",
    "# and map them to their category names (word, non-word)\n",
    "categories = {}\n",
    "for cat in [\"words\", \"non-words\"]:\n",
    "    with open(os.path.join(\"..\", \"data\", \"samples-databases\", cat + \".txt\")) as f:\n",
    "        for w in f.readlines():\n",
    "            categories[w.strip()] = cat[:-1] # remove the final \"s\"\n",
    "\n",
    "# add missing data & evaluate responses\n",
    "for i, rd in d.iterrows():\n",
    "    # convert string \"yes\" to boolean\n",
    "    rd[\"Fluent\"] = (rd[\"Fluent\"] == \"yes\")\n",
    "    if isinstance(rd[\"Category\"], float) or rd[\"Category\"] is np.nan:\n",
    "        # assing correct category if missing\n",
    "        rd[\"Category\"] = categories[rd[\"Sample\"]]\n",
    "    # set Correct to 1 when the participant said sure or probably\n",
    "    # set to zero otherwise\n",
    "    rd[\"Correct\"] = 0\n",
    "    if rd[\"Type\"] == \"lexical\":\n",
    "        if rd[\"Response\"] == (\"Sure \" + rd[\"Category\"]) or \\\n",
    "          rd[\"Response\"] == (\"Probably \" + rd[\"Category\"]):\n",
    "            rd[\"Correct\"] = 1\n",
    "    elif rd[\"Type\"] == \"recognition\":\n",
    "        if rd[\"Response\"] == (\"Sure \" + rd[\"Seen\"]) or \\\n",
    "          rd[\"Response\"] == (\"Probably \" + rd[\"Seen\"]):\n",
    "            rd[\"Correct\"] = 1\n",
    "    d.loc[i] = rd\n",
    "            \n",
    "# add normalized RT\n",
    "d[\"RTnorm\"] = normalize_rt(d[\"RT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data for each part and participant\n",
    "\n",
    "Aggregate data for every (study, test, participant) combination. This results in four rows per participant:\n",
    "- TaskID 1 for: part/test 1, lexical task\n",
    "- TaskID 2 for: part/test 1, recognition task\n",
    "- TaskID 3 for: part/test 2, lexical task\n",
    "- TaskID 4 for: part/test 2, recognition task\n",
    "\n",
    "Calculate AUC and mean RT across all of their relevant responses\n",
    "and get them for: all, words, and non-words, normalized and not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_results(d_):\n",
    "    \"\"\"\n",
    "    Aggregate data for every (study, test, participant) combination.\n",
    "    \"\"\"\n",
    "    \n",
    "    d = d_.copy()\n",
    "\n",
    "    # Prefill results\n",
    "    # aggregate correct-ness and response times (use mean value)\n",
    "    # keep the rest as is or set NaN value for new columns\n",
    "    result_columns = [\"StudyID\", \"ParticipantID\", \"TestID\", \"TaskID\", \"Type\", \"Order\", \"Ordertype\",\n",
    "                      \"Firstfont\",\n",
    "                      \"Fluent\", \"Training\", \"isDesigner\", \"Font\", \"Correct\", \"RT\", \"RTnorm\",\n",
    "                      \"RT_word\", \"RT_nonword\", \"RTnorm_word\", \"RTnorm_nonword\",\n",
    "                      \"AUC\", \"AUC_word\", \"AUC_nonword\",\n",
    "                      \"JoL\", \"JoM\", \"Date\"]\n",
    "    agg_columns = {k:\"first\" for k in set(d.columns).intersection(result_columns)}\n",
    "    agg_columns[\"Correct\"] = \"mean\"\n",
    "    agg_columns[\"RT\"] = \"mean\"\n",
    "    agg_columns[\"RTnorm\"] = \"mean\"\n",
    "    results = d.groupby([\"StudyID\", \"ParticipantID\", \"TestID\", \"Type\"]).agg(agg_columns)\n",
    "    results = pd.DataFrame(results, columns=result_columns)\n",
    "    results.set_index([\"StudyID\", \"ParticipantID\", \"TestID\", \"Type\"], inplace=True)\n",
    "    # isDesigner is a boolean column to conveniently group designers together\n",
    "    results[\"isDesigner\"] = (results[\"Training\"] != \"Non-designer\")\n",
    "    # convert JoL responses to numerical values\n",
    "    for k, v in map_JoL.items():\n",
    "        results[\"JoL\"] = results[\"JoL\"].astype(str).replace(k, v)\n",
    "    results[\"JoL\"] = results[\"JoL\"].astype(float)\n",
    "\n",
    "    test_ids = sorted(set(d[\"TestID\"].unique()))\n",
    "    ttypes = sorted(d[\"Type\"].unique())\n",
    "    \n",
    "    # Prepare indexes for temporary data frames used to calculate the AUC.\n",
    "    # There are two indexes, one based on the “Category” column used for lexical tasks\n",
    "    # and one based on the “Seen” column used for recognition.\n",
    "    ix = {}\n",
    "    category_name = \"Category\"\n",
    "    categories = [\"word\", \"non-word\"]\n",
    "    responses = [\"Sure word\", \"Probably word\", \"Probably non-word\", \"Sure non-word\"]\n",
    "    ix[\"lexical\"] = (category_name, pd.MultiIndex.from_product([categories, responses], names=[category_name, \"Response\"]))\n",
    "    category_name = \"Seen\"\n",
    "    categories = [\"seen\", \"not seen\"]\n",
    "    responses = [\"Sure seen\", \"Probably seen\", \"Probably not seen\", \"Sure not seen\"]\n",
    "    ix[\"recognition\"] = (category_name, pd.MultiIndex.from_product([categories, responses], names=[category_name, \"Response\"]))\n",
    "\n",
    "    # Get each part/test separately\n",
    "    for sid in d[\"StudyID\"].unique():\n",
    "        for pid in d[d[\"StudyID\"] == sid][\"ParticipantID\"].unique():\n",
    "            taskid = 0\n",
    "            for tid in test_ids:\n",
    "                for order, ttype in enumerate(ttypes):\n",
    "                    order += 1  # (0, 1) -> (1, 2)\n",
    "                    taskid  += 1 # 2 * (int(tid) - 1) + order # -> (1, 2, 3, 4)\n",
    "                    \n",
    "                    # Subset the data frame to single task-type combination\n",
    "                    # there are four (two parts/tests with two tasks) for each participant in a study\n",
    "                    dtt = d[(d[\"StudyID\"] == sid) & (d[\"ParticipantID\"] == pid) & (d[\"TestID\"] == tid) & (d[\"Type\"] == ttype)]\n",
    "                    # get/save the order\n",
    "                    if sid == 1:\n",
    "                        # in study #1 it corresponds to 1 = lexical, 2 = recognition\n",
    "                        results.loc[(sid, pid, tid, ttype), \"Order\"] = order\n",
    "                    else:\n",
    "                        # in study #2 it depends on the Test ID\n",
    "                        if tid == 1:\n",
    "                            results.loc[(sid, pid, tid, ttype), \"Order\"] = order\n",
    "                        elif tid == 2 and order == 1:\n",
    "                            results.loc[(sid, pid, tid, ttype), \"Order\"] = 2\n",
    "                        elif tid == 2 and order == 2:\n",
    "                            results.loc[(sid, pid, tid, ttype), \"Order\"] = 1\n",
    "                        else:\n",
    "                            print(\"This should not happen\", tid, order)\n",
    "                    results.loc[(sid, pid, tid, ttype), \"TaskID\"] = taskid\n",
    "                    ffont = results.loc[(sid, pid, tid, ttype), \"Firstfont\"]\n",
    "                    results.loc[(sid, pid, tid, ttype), \"Ordertype\"] = int(ffont == \"arial\") + 1\n",
    "                    \n",
    "                    # Calculate the AUC\n",
    "                    # figure out which category and index to use for this test type\n",
    "                    # category_name is either “Category” (in lexical task) or “Seen” (in recognition task)\n",
    "                    category_name, index = ix[ttype]\n",
    "                    # get response frequencies for this test type first\n",
    "                    # ensure the order in the index is always the same\n",
    "                    dg = pd.DataFrame(index=index)\n",
    "                    dg[\"Frequencies\"] = dtt.groupby([category_name])[\"Response\"].value_counts()\n",
    "                    dg = dg.fillna(0)\n",
    "                    # use frequencies for word/seen for the y coordinate in the get_auc function\n",
    "                    # use frequencies for non-word/not seen for the x coordinate\n",
    "                    freqs = dg[\"Frequencies\"].tolist()\n",
    "                    auc = get_auc(freqs[4:], freqs[:4])\n",
    "                    results.loc[(sid, pid, tid, ttype), \"AUC\"] = auc\n",
    "                    results.loc[(sid, pid, tid, ttype), \"AUCnorm\"] = normalize_auc(auc)\n",
    "                    correct = results.loc[(sid, pid, tid, ttype), \"Correct\"]\n",
    "                    results.loc[(sid, pid, tid, ttype), \"Correctnorm\"] = normalize_auc(correct)\n",
    "\n",
    "                    # For recognition task only\n",
    "                    # get the mean AUC and RT for words only and non-words only\n",
    "                    if ttype == \"recognition\":\n",
    "                        for cat in [\"word\", \"non-word\"]:\n",
    "                            cat_ = cat.replace(\"-\", \"\")\n",
    "                            dg[\"Frequencies\"] = dtt[dtt[\"Category\"] == cat].groupby([category_name])[\"Response\"].value_counts()\n",
    "                            dg = dg.fillna(0)\n",
    "                            freqs = dg[\"Frequencies\"].tolist()\n",
    "                            auc = get_auc(freqs[4:], freqs[:4])\n",
    "                            results.loc[(sid, pid, tid, ttype), \"AUC_%s\" % cat_] = auc\n",
    "                            results.loc[(sid, pid, tid, ttype), \"AUCnorm_%s\" % cat_] = normalize_auc(auc)\n",
    "                        rt = dtt[dtt[\"Category\"] == cat][\"RT\"].mean()\n",
    "                        results.loc[(sid, pid, tid, ttype), \"RT_%s\" % cat_] = rt\n",
    "                        results.loc[(sid, pid, tid, ttype), \"RTnorm_%s\" % cat_] = normalize_rt(rt)                    \n",
    "            results.loc[(sid, pid), \"Firstfont\"] = results.loc[(sid, pid, 1, \"lexical\"), \"Font\"]\n",
    "    # fix the type for order column\n",
    "    # results[\"Order\"] = results[\"Order\"].astype(int)\n",
    "    # swap recognition tests for SID == 2\n",
    "    # to have lexical and recognition next to each other\n",
    "    for pid in d[d[\"StudyID\"] == 2][\"ParticipantID\"].unique():\n",
    "        backup = results.loc[(2, pid, 1, \"recognition\")].copy()\n",
    "        results.loc[(2, pid, 1, \"recognition\")] = results.loc[(2, pid, 2, \"recognition\")]\n",
    "        results.loc[(2, pid, 2, \"recognition\")] = backup\n",
    "    results.reset_index(inplace=True)\n",
    "    # fix column type\n",
    "    results[\"TaskID\"] = results[\"TaskID\"].astype(\"int\")\n",
    "    results[\"Ordertype\"] = results[\"Ordertype\"].astype(\"int\")\n",
    "    return results\n",
    "\n",
    "aggregated = get_agg_results(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved to CSV files.\n"
     ]
    }
   ],
   "source": [
    "# save the processed and aggregated data\n",
    "d.to_csv(os.path.join(\"..\", \"data\", \"data.csv\"))\n",
    "aggregated.to_csv(os.path.join(\"..\", \"data\", \"data_aggregated.csv\"))\n",
    "print(\"Successfully saved to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not used: remove outliers\n",
    "\n",
    "Outliers are responses times (RTs) outside mean +- 2*STD.\n",
    "These RTs are being replaced by a mean of RTs for a participant and task type (lexical, decision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 557 outliers\n",
      "Replacing outliers changes the global normalized mean from 7.756 to 7.719\n",
      "Replacing outliers changes the global mean from 4017.465 to 2642.310\n"
     ]
    }
   ],
   "source": [
    "def replace_outliers(d_):\n",
    "    \"\"\"\n",
    "    Replace responses outside mean +- 2*STD with a mean\n",
    "    \"\"\"\n",
    "    \n",
    "    d= d_.copy()\n",
    "    means = d.groupby([\"ParticipantID\", \"Type\"])[\"RTnorm\"].mean()\n",
    "    stds = d.groupby([\"ParticipantID\", \"Type\"])[\"RTnorm\"].std()\n",
    "    meanorigs = d.groupby([\"ParticipantID\", \"Type\"])[\"RT\"].mean()\n",
    "    d.set_index([\"ParticipantID\", \"Type\", \"TestID\", \"TrialID\"], inplace=True)\n",
    "    count = 0\n",
    "    # traverse individual responses\n",
    "    for ix, dt in d.iterrows():\n",
    "        # get mean and std for participant and task type\n",
    "        mean = means[ix[:-2]]\n",
    "        std = stds[ix[:-2]]\n",
    "        meanorig = meanorigs[ix[:-2]]\n",
    "        # judge by RTnorm, but replace both RTnorm and RT\n",
    "        if dt[\"RTnorm\"] >= (mean + 2*std):\n",
    "            count += 1\n",
    "            d.loc[ix, \"RT\"] = meanorig\n",
    "            d.loc[ix, \"RTnorm\"] = normalize_rt(meanorig)\n",
    "    d.reset_index(inplace=True)\n",
    "    print(\"Replaced %d outliers\" % count)\n",
    "    return d\n",
    "\n",
    "dwo = replace_outliers(d)\n",
    "aggregatedwo = get_agg_results(dwo)\n",
    "\n",
    "print(\"Replacing outliers changes the global normalized mean from %.3f to %.3f\" % (d[\"RTnorm\"].mean(), dwo[\"RTnorm\"].mean()))\n",
    "print(\"Replacing outliers changes the global mean from %.3f to %.3f\" % (d[\"RT\"].mean(), dwo[\"RT\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved to CSV files.\n"
     ]
    }
   ],
   "source": [
    "# save the processed and aggregated data\n",
    "dwo.to_csv(os.path.join(\"..\", \"data\", \"data_outliers-replaced.csv\"))\n",
    "aggregatedwo.to_csv(os.path.join(\"..\", \"data\", \"data_outliers-replaced_aggregated.csv\"))\n",
    "print(\"Successfully saved to CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
